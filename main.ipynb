{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story added to memory.\n",
      "Response structure: generations=[[Generation(text='Leonardo da Vinci')]] llm_output=None run=[RunInfo(run_id=UUID('2a228dac-a455-467c-bf03-4e271209003a'))]\n",
      "Chatbot: Leonardo da Vinci\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI\n",
    "\n",
    "# Configurez votre clé API Google Generative AI ici\n",
    "GOOGLE_API_KEY = 'AIzaSyAD35w9sxvo7DTnL85e0F5BsBsTH60g8xY'\n",
    "\n",
    "# Initialiser les embeddings et le modèle de langage Google Generative AI\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "llm = GoogleGenerativeAI(model=\"models/text-bison-001\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Supposons que la dimension des embeddings soit 768 (à ajuster selon votre modèle)\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "# Classe pour gérer la mémoire du chatbot avec FAISS\n",
    "class ChatbotMemory:\n",
    "    def __init__(self, embeddings, embedding_dim):\n",
    "        self.embeddings = embeddings\n",
    "        self.index = faiss.IndexFlatIP(embedding_dim)\n",
    "        self.memory = {}\n",
    "\n",
    "    def remember(self, user_id, story):\n",
    "        if user_id not in self.memory:\n",
    "            self.memory[user_id] = []\n",
    "        self.memory[user_id].append(story)\n",
    "\n",
    "        # Obtenir l'embedding pour l'histoire et l'indexer avec FAISS\n",
    "        embedding = self.embeddings.embed_query(story)\n",
    "        embedding = np.array([embedding]).astype('float32')  # FAISS nécessite des float32\n",
    "        self.index.add(embedding)\n",
    "\n",
    "    def recall(self, user_id, query, k=5):\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        query_embedding = np.array([query_embedding]).astype('float32')  # FAISS nécessite des float32\n",
    "        \n",
    "        # Rechercher les embeddings les plus similaires dans l'index FAISS\n",
    "        D, I = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Récupérer les histoires correspondantes aux indices I\n",
    "        similar_stories = [self.memory[user_id][i] for i in I[0] if i < len(self.memory[user_id])]\n",
    "        return similar_stories\n",
    "\n",
    "memory = ChatbotMemory(embeddings, EMBEDDING_DIM)\n",
    "\n",
    "# Fonction pour ajouter une histoire à la mémoire du chatbot\n",
    "def add_story(user_id, story):\n",
    "    memory.remember(user_id, story)\n",
    "    return \"Story added to memory.\"\n",
    "\n",
    "# Fonction pour interroger le chatbot basé sur les histoires en mémoire\n",
    "def ask_question(user_id, question):\n",
    "    history = memory.recall(user_id, question)\n",
    "    \n",
    "    # Créer une invite de chat basée sur les histoires similaires et la question actuelle\n",
    "    prompt = f\"Based on the following stories: {'. '.join(history)}. User's question: {question}. Answer:\"\n",
    "    \n",
    "    # Générer une réponse basée sur le contexte\n",
    "    response = llm.generate(prompts=[prompt])  # Notez que nous passons une liste de chaînes de caractères\n",
    "    \n",
    "    # Inspecter la structure de la réponse pour le débogage\n",
    "    print(\"Response structure:\", response)\n",
    "    \n",
    "    # Récupérer le texte généré à partir de la réponse\n",
    "    try:\n",
    "        # Tentative d'accès aux générations dans la réponse\n",
    "        generated_text = response.generations[0][0].text\n",
    "    except AttributeError as e:\n",
    "        print(f\"Attribute error: {e}\")\n",
    "        generated_text = \"Error generating response.\"\n",
    "    except IndexError as e:\n",
    "        print(f\"Index error: {e}\")\n",
    "        generated_text = \"Error generating response.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Other error: {e}\")\n",
    "        generated_text = \"Error generating response.\"\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Exemple d'utilisation\n",
    "user_id = \"user123\"\n",
    "story = \"My name is Mona Lisa, I was painted by Leonardo da Vinci, is a masterpiece renowned for its enigmatic expression and pioneering use of sfumato.\"\n",
    "question = \"By whom was the Mona Lisa painted?\"\n",
    "\n",
    "# Ajouter une histoire à la mémoire\n",
    "print(add_story(user_id, story))\n",
    "\n",
    "# Poser une question basée sur l'histoire en mémoire\n",
    "response = ask_question(user_id, question)\n",
    "print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
